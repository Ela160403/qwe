# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_F3biAWMOaZsL4NkKm0MGunkx0ttEGs
"""

import numpy as np
class Perceptron(object):
    # Initialize the perceptron with weights, learning rate, and epochs
    def __init__(self, input_size, lr=0.2, epochs=4):
        self.W = np.array([0.3, -0.2])  # Initial weights
        self.epochs = epochs  # Number of training cycles
        self.lr = lr  # Learning rate
    # Activation function (step function)
    def activation_fn(self, x):
        return 1 if x >= 0 else 0
    # Predict function calculates weighted sum and applies activation function
    def predict(self, x, theta):
        z = self.W.T.dot(x) - theta  # Weighted sum minus bias
        z = round(z, 2)  # Round to 2 decimal places
        a = self.activation_fn(z)  # Apply step function
        return a
    # Train the perceptron
    def fit(self, X, d, theta, count):
        for _ in range(self.epochs):  # Loop for the number of epochs
            print("Epoch: ", count)
            count += 1
            for i in range(d.shape[0]):  # Loop through each input
                x = X[i]
                print("Input:", x, "\tWeight:", self.W)
                y = self.predict(x, theta)  # Predicted output
                e = d[i] - y  # Error (desired - predicted)
                self.W = self.W + self.lr * e * x  # Update weights
# Main function
if __name__ == '__main__':
    # Input dataset (AND logic)
    X = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ])
    d = np.array([0, 0, 0, 1])  # Desired output (AND logic)
    # Initialize perceptron
    perceptron = Perceptron(input_size=2)
    theta = 0.4  # Bias threshold
    count = 1  # Epoch counter
    # Train the perceptron
    perceptron.fit(X, d, theta, count)
    # Print final weights after training
    print("Final Weights:", perceptron.W)

import pandas as pd
from scipy import stats
import numpy as np

# Generate dataset with random Marks
np.random.seed(42)  # Ensures reproducibility
df = pd.DataFrame({
    "id": range(1, 11),
    "first": ["Leone", "Romola", "Geri", "Sandy", "Jacenta", "Diane-marie", "Austen", "Vanya", "Giordano", "Rozele"],
    "last": ["Debrick", "Phinnessy", "Prium", "Doveston", "Jansik", "Medhurst", "Pool", "Teffrey", "Elloy", "Fawcett"],
    "gender": ["Female", "Female", "Male", "Female", "Female", "Female", "Male", "Male", "Male", "Female"],
    "Marks": np.random.randint(30, 100, size=10)  # Random Marks between 30 and 100
})

# Add 'selected' column based on Marks
df["selected"] = df["Marks"] > 50

# Display the dataset
print("Dataset:\n", df)

# Basic descriptive statistics
print("\nBasic Descriptive Statistics:")
print(f"Mean: {df['Marks'].mean():.2f}")
print(f"Sum: {df['Marks'].sum()}")
print(f"Max: {df['Marks'].max()}")
print(f"Min: {df['Marks'].min()}")
print(f"Count: {df['Marks'].count()}")
print(f"Median: {df['Marks'].median():.2f}")
print(f"Standard Deviation: {df['Marks'].std():.2f}")
print(f"Variance: {df['Marks'].var():.2f}")

# Use describe() for summary
print("\nSummary using describe():")
print(df.describe())

# Advanced statistics using SciPy
data = df['Marks'].tolist()
print("\nAdvanced Statistics:")
print("Details:", stats.describe(data))
print(f"Geometric Mean: {stats.gmean(data):.2f}")
print(f"Harmonic Mean: {stats.hmean(data):.2f}")
print(f"Interquartile Range (IQR): {stats.iqr(data):.2f}")
print(f"Skewness: {stats.skew(data):.2f}")
print(f"Kurtosis: {stats.kurtosis(data):.2f}")

# Spearman correlation example
data1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Example dataset for correlation
print("\nSpearman Correlation:", stats.spearmanr(data, data1))

import pandas as pd
import numpy as np
from scipy import stats
np.random.seed(42)
df=pd.DataFrame({
    "name":["av","ac","ac"],
    "marks":np.random.randint(30,100, size=3),
    "gender":np.random.choice(["male","female"],size=3)
})
print(df)
df["selected"]=df["marks"]>50
print("mean:",df["marks"].describe(),"\n")
print(df)
data=[1,2,3,4,5,6,7,56,8,9,6]
print(stats.hmean(data))
print("mean is:",df["marks"].mean())

import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, Binarizer
from sklearn import preprocessing

# Load dataset
columns = ["id", "first", "last", "gender", "Marks", "selected"]

data = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'first': ['Leone', 'Romola', 'Geri', 'Sandy', 'Jacenta', 'Diane-marie', 'Austen', 'Vanya', 'Giordano', 'Rozele'],
    'last': ['Debrick', 'Phinnessy', 'Prium', 'Doveston', 'Jansik', 'Medhurst', 'Pool', 'Teffrey', 'Elloy', 'Fawcett'],
    'gender': ['Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female'],
    'Marks': [50, 60, 65, 95, 31, 45, 45, 70, 36, 50]
})

data["Selected"]=data["Marks"]>50


# Label Encoding: Convert gender to 0 for Female and 1 for Male
data['gender'] = LabelEncoder().fit_transform(data['gender'])
print("After Label Encoding:\n", data)

# Scaling Marks (removes mean)
data['Marks'] = preprocessing.scale(data['Marks'])
print("\nAfter Scaling Marks:\n", data)

# Binarizing Marks with threshold 0.5
marks_array = data['Marks'].values.reshape(-1, 1)
data['Marks'] = Binarizer(threshold=0.5).transform(marks_array)
print("\nAfter Binarizing Marks:\n", data)

# Create duplicate data and remove duplicates
duplicated_data = pd.concat([data] * 2, ignore_index=True)
data_without_duplicates = duplicated_data.drop_duplicates()
print("\nAfter Removing Duplicates:\n", data_without_duplicates)

# Handle NaN values: Fill with 0 or mean or median
data['Marks'] = data['Marks'].apply(pd.to_numeric, errors='coerce')
data['Marks'] = data['Marks'].fillna(0)
print("\nAfter Filling NaN with 0:\n", data)

data['Marks'] = data['Marks'].fillna(data['Marks'].mean())
print("\nAfter Filling NaN with Mean:\n", data)

# Drop columns with NaN values
cleaned_data = data.dropna(axis=1)
print("\nAfter Dropping Columns with NaN:\n", cleaned_data)

# Scaling with MinMax and Standard Scaler
data_points = [[1, 3], [8, 5], [6, 7], [8, 9]]

scaled_minmax = MinMaxScaler().fit_transform(data_points)
scaled_standard = StandardScaler().fit_transform(data_points)

print("\nMinMax Scaling:\n", scaled_minmax)
print("\nStandard Scaling (Z-scores):\n", scaled_standard)

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression

# Simple dataset: week and sales data
salesdata = {
    'week': [1, 2, 3, 4, 5],         # Week numbers
    'sales': [1.2, 1.8, 2.6, 3.2, 3.8]  # Sales in thousands
}

# Creating a DataFrame from the data
df = pd.DataFrame(salesdata)

# Plotting the data to visualize the relationship between 'week' and 'sales'
plt.scatter(df['week'], df['sales'], color='green')
plt.title('Week vs Sales Regression')
plt.xlabel('Week')
plt.ylabel('Sales in Thousands')
plt.show()

# X is the feature (week), y is the target (sales)
X = df[['week']]  # Features (week)
y = df['sales']   # Target (sales)

# Creating a linear regression model
regr = LinearRegression()

# Fitting the model with the data
regr.fit(X, y)

# Printing the intercept and coefficients of the regression line
print('Intercept (b):', regr.intercept_)
print('Coefficient (m):', regr.coef_)

# Regression equation: y = m * X + b
print('\nThe Regression Equation is: y =', regr.coef_[0], '* X +', regr.intercept_)

# Predicting the sales for the given week data
pred = regr.predict(X)

# Plotting the regression line
plt.plot(df['week'], pred, color='blue')
plt.scatter(df['week'], df['sales'], color='green')  # Scatter plot of original data
plt.title('Week vs Sales with Regression Line')
plt.xlabel('Week')
plt.ylabel('Sales in Thousands')
plt.show()

# Computing the R-squared value for model performance
print("\nR-squared (Goodness of Fit):", regr.score(X, y))

import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression

# Creating a random dataset with 2 features and 50 samples
X, y = make_regression(n_samples=50, n_features=2, noise=0.1)

# Plotting the data for visualization
plt.scatter(X[:, 0], y, color='green')
plt.title('Multiple Regression: Feature 1 vs Target')
plt.xlabel('Feature 1')
plt.ylabel('Target')
plt.show()

# Creating the regression model
regr = LinearRegression()

# Fitting the model with the data (multiple features)
regr.fit(X, y)

# Printing the intercept and coefficients for the multiple regression
print('Intercept:', regr.intercept_)
print('Coefficients:', regr.coef_)

# Regression equation: y = b + m1*X1 + m2*X2
print('\nThe Regression Equation is: y =', regr.intercept_, '+', regr.coef_[0], '* X1 +', regr.coef_[1], '* X2')

# Predicting the target values based on the features
pred = regr.predict(X)

# Plotting the regression line (only for first feature and target)
plt.plot(X[:, 0], pred, color='blue')
plt.scatter(X[:, 0], y, color='green')
plt.title('Multiple Regression: Feature 1 vs Target with Regression Line')
plt.xlabel('Feature 1')
plt.ylabel('Target')
plt.show()

# Computing the R-squared value for model performance
print("\nR-squared (Goodness of Fit):", regr.score(X, y))

import pandas as pd
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Step 1: Create a random dataset with 3 centers and 3 features
X, y = make_blobs(n_samples=200, centers=3, n_features=3, random_state=42)

# Step 2: Convert the dataset into a pandas DataFrame for easy viewing
df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2', 'Feature 3'])
df['Label'] = y  # Adding the target variable (Label) to the DataFrame

# Display the top 5 rows of the dataset
print("Top five Records\n")
print(df.head())  # Show the first 5 rows of the DataFrame

# Step 3: Split the data into training and testing sets (60% training, 40% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=0)

# Step 4: Initialize the Logistic Regression model
model = LogisticRegression()

# Step 5: Train the model on the training data
model.fit(X_train, y_train)

# Step 6: Make predictions on the test data
predicted = model.predict(X_test)

# Step 7: Print the classification report to evaluate the model's performance
print("\nClassification Report")
print(classification_report(y_test, predicted))

import pandas as pd

# Define the dataset directly in the code
data = {
    'CGPA': ['g9', 'g9', 'g8', 'g9'],
    'Interactiveness': ['Yes', 'Yes', 'No', 'Yes'],
    'Practical Knowledge': ['Excellent', 'Good', 'Good', 'Good'],
    'Communication Skills': ['Good', 'Good', 'Good', 'Good'],
    'Logical Thinking': ['Fast', 'Fast', 'Fast', 'Slow'],
    'Interest': ['Yes', 'Yes', 'No', 'No'],
    'Job Offer': ['Yes', 'Yes', 'No', 'Yes']
}

# Create the DataFrame
df = pd.DataFrame(data)

# Print the training dataset
print("\nTraining Dataset")
print(df)

# The number of attributes (excluding 'Job Offer')
num_attrs = len(df.columns) - 1  # Excluding the 'Job Offer' column

# The most general and specific hypothesis
print("\nThe most general hypothesis : ['?','?','?','?','?','?','?']\n")
print("\nThe most specific hypothesis : ['0','0','0','0','0','0','0']\n")

# Initialize the hypothesis list
h = ['0'] * num_attrs
print("\nThe initial value of hypothesis: ")
print(h)

# Generate the hypothesis 'h' with the first training instance
for j in range(num_attrs):
    h[j] = df.iloc[0, j]  # Assign the values from the first row

# Generate hypothesis 'h' with subsequent training instances
print("\nFind S: Finding Maximally Specific Hypothesis")
for i in range(len(df)):
    if df.iloc[i, num_attrs] == 'Yes':  # 'Job Offer' is 'Yes'
        for j in range(num_attrs):
            if df.iloc[i, j] != h[j]:
                h[j] = '?'  # Make it more general if the values don't match
            else:
                h[j] = df.iloc[i, j]  # Keep the same if they match
    print(f"Training Instance {i}: the hypothesis 'h' is {h}")

# Print the final maximally specific hypothesis
print("\nThe Final Maximally Specific Hypothesis 'h' is:")
print(h)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Create a random dataset with sample data
np.random.seed(42)  # For reproducibility

# Number of records
num_records = 50

# Generate random data for columns
data = {
    'id': np.arange(1, num_records + 1),  # Student IDs (1 to 50)
    'name': [f'Student_{i}' for i in range(1, num_records + 1)],  # Student names
    'gender': np.random.choice(['Male', 'Female'], num_records),  # Random gender
    'physics': np.random.randint(0, 101, num_records),  # Random physics marks between 0 and 100
    'chemistry': np.random.randint(0, 101, num_records),  # Random chemistry marks between 0 and 100
    'maths': np.random.randint(0, 101, num_records),  # Random maths marks between 0 and 100
    'biology': np.random.randint(0, 101, num_records),  # Random biology marks between 0 and 100
    'language': np.random.randint(0, 101, num_records),  # Random language marks between 0 and 100
    'selected': np.random.choice([0, 1], num_records)  # Randomly selected (0 = No, 1 = Yes)
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Display the DataFrame
print(df.head())


# --- Univariate Graphs ---

# 1. Histogram for Physics Marks
plt.hist(df['physics'], bins=10, color='skyblue', edgecolor='black')  # Create a histogram
plt.title('Histogram for Physics Marks')
plt.xlabel('Physics Marks')
plt.ylabel('Frequency')
plt.show()  # Display the plot

# --- Bivariate Graphs ---

# 2. Scatter plot for Chemistry vs Physics Marks
plt.scatter(df['chemistry'], df['physics'], alpha=0.5, color='orange')  # Scatter plot
plt.title('Scatter plot: Chemistry vs Physics')
plt.xlabel('Chemistry Marks')
plt.ylabel('Physics Marks')
plt.show()

# --- Box Plot ---

# 3. Box Plot of Marks for All Subjects
df[['physics', 'chemistry', 'maths', 'biology', 'language']].plot.box(title="Box and Whisker Plot of Marks", grid=False)
plt.show()  # Display the plot

# --- Pie Chart ---

# 4. Pie Chart of Selected Students by Gender
gender_selected = df.groupby('gender')['selected'].sum()  # Group by gender and sum the selections
plt.pie(gender_selected, labels=gender_selected.index, autopct='%1.1f%%', colors=['lightblue', 'lightgreen'])
plt.title('Selected Students by Gender')
plt.show()

# --- Multiplots ---

# 5. Multiple Box Plots for All Subjects
df[['physics', 'chemistry', 'maths', 'biology', 'language']].plot(kind='box', subplots=True, layout=(3, 3), sharex=False, sharey=False)
plt.suptitle('Box Plots for All Subjects')
plt.show()

# --- Scatter Matrix ---

# 6. Scatter Matrix for Subjects (Excluding Non-Numerical Columns)
from pandas.plotting import scatter_matrix
scatter_matrix(df[['physics', 'chemistry', 'maths', 'biology', 'language']], alpha=0.2, figsize=(10, 10), diagonal='hist')
plt.show()

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression

# Generate random data for 100 students
np.random.seed(56)  # For reproducibility
num_records = 10000000

# Generate random data for the 5 subjects
data = {
    'tam': np.random.randint(40, 101, num_records),  # Tamil marks
    'eng': np.random.randint(40, 101, num_records),  # English marks
    'math': np.random.randint(40, 101, num_records),  # Math marks
    'phy': np.random.randint(40, 101, num_records),  # Physics marks
    'chem': np.random.randint(40, 101, num_records),  # Chemistry marks
}

# Create the DataFrame
df = pd.DataFrame(data)

# Create the 'result' column: True if marks in all subjects are greater than 50, else False
df['result'] = (df[['tam', 'eng', 'math', 'phy', 'chem']] > 50).all(axis=1).astype(int)

# Display the first few rows of the DataFrame
print(df.head())

# Prepare the data for regression
x = df[['tam', 'eng', 'math', 'phy', 'chem']]  # Independent variables (5 subjects)
y = df['result']  # Dependent variable (result)

# Initialize Linear Regression model
regr1 = LogisticRegression()

# Fit the model
regr1.fit(x, y)

# Predict the result for a new dataset (e.g., tam=56, eng=66, math=70, phy=80, chem=75)
prediction = regr1.predict([[56, 66, 70, 80, 75]])

# Display the prediction
print(f"Predicted result for the input [56, 66, 70, 80, 75]: {prediction[0]}")

